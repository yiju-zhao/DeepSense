from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import contextmanager
from datetime import date, datetime
from enum import Enum
import json
import logging
import math
import os
from pathlib import Path
import re
from string import Template
import sys
import time
from typing import Dict, List

import fitz
from openai import OpenAI, Timeout
import requests
from dotenv import load_dotenv
from config import get_data_storage_dir
from database import SessionLocal
from models.tasks import ArxivPaper, PaperScores, Publication, SOTAContext
from sqlalchemy.exc import SQLAlchemyError

# Load environment variables from .env file
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(funcName)s[%(lineno)d] - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout), logging.FileHandler("app.log")],
)
# Create logger for this module
logger = logging.getLogger(__name__)


class InMemoryCacheDevOnly:
    def __init__(self):
        self.cache = {}

    def get(self, prompt: str):
        """
        Retrieve the cached response for the given prompt.
        Returns the JSON response if found, or None if not cached.
        """
        return self.cache.get(prompt)

    def set(self, prompt: str, response: dict):
        """
        Cache the JSON response for the given prompt.
        """
        self.cache[prompt] = response


cache = InMemoryCacheDevOnly()


class BaseAssistant:
    def __init__(self, config: dict):
        self.name = config.get("name")
        self.model_name = config.get("model_name")
        self.prompt = config.get("prompt")
        self.instruction = config.get("instruction")
        # Get API key from environment variables (loaded from .env file)
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            logger.warning("OPENAI_API_KEY not found in environment variables")
        self.client = OpenAI(api_key=api_key)

    def _get_db(self):
        db = SessionLocal()
        try:
            yield db
        finally:
            db.close()

    def do_work(self, publication: Publication, context: dict) -> dict:
        return self._get_response(publication=publication, prompt=self.prompt)

    def ask_question(self, publication: Publication, prompt: str) -> dict:
        # è¿½é—®æ›´å¤šé—®é¢˜
        return self._get_response(publication=publication, prompt=prompt)

    def _get_response(self, publication: Publication, prompt: str) -> dict:
        max_retries = 2  # å…è®¸é‡è¯•2æ¬¡
        cached_response = cache.get(prompt)
        if cached_response:
            logger.info(f"!!biggo!!Cached response return: {cached_response}")
            return cached_response

        for attempt in range(max_retries + 1):
            try:
                # è°ƒç”¨ OpenAI æ¥å£
                response = self.client.responses.create(
                    model=self.model_name,
                    instructions=self.instruction,
                    input=prompt,
                    max_output_tokens=10000,  # TODO: è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…éœ€è¦è°ƒæ•´,å¼€å‘é˜¶æ®µï¼Œé™åˆ¶é•¿åº¦
                )
                # è§£æå¹¶è¿”å›ç»“æœ
                result = self._parse_response(response)
                # TODO: ä¼˜åŒ–å®ƒ
                cache.set(prompt=prompt, response=result)
                return result
            except ValueError as ve:
                logger.error(
                    f"Recived an invalid response from LLM API call\n Response:{response} \n and the errors are:\n {ve}"
                )
                logger.info(f"Retrying {attempt + 1}...")
            except Exception as e:
                logger.error(
                    f"Recived an invalid response from LLM API call\n Response:{response} \n and the errors are:\n {ve}"
                )
        # failed retry:
        logger.error(f"Attempt {attempt + 1} failed. Exit with error!!!")
        return None

    def _load_domain_sota_knowledge(self, publication: Publication) -> str:
        if publication.research_topics:
            keywords = publication.research_topics
        elif publication.keywords:
            keywords = publication.keywords
        else:
            keywords = (
                "I dont have keywords, but I have the title: " + publication.title
            )
        # è®©AIæ¥æ€»ç»“æ–‡ç« çš„ç ”ç©¶æ–¹å‘/å…³é”®æŠ€æœ¯ï¼Œè¿™ä¸ªä¸æ€»æ˜¯ç­‰äºå…³é”®å­—, åº”è¯¥è¦æ¯”å…³é”®å­—æ›´åŠ å…·ä½“ã€‚ æ¯”å¦‚â€œkv cacheçš„å‹ç¼©â€
        # set default value:
        sota_result = f"Remember, you are the best expector in this domain. \
                I can not provide enough backgroud knowledge context to you, \
                please try your best based on your memory.\n"
        if keywords:
            db = next(self._get_db())
            sota_context_list = []
            # æ ¹æ®top 3 çš„å…³é”®å­—ï¼ˆèŠ‚çº¦ä¸Šä¸‹æ–‡ï¼‰ï¼Œå…ˆä»æ•°æ®åº“ä¸­è·å–ç›¸å…³çš„çŸ¥è¯†
            for keyword in keywords[:3]:
                sota_context = (
                    db.query(SOTAContext).filter(SOTAContext.keyword == keyword).first()
                )
                if sota_context and sota_context.research_context:
                    sota_context_list.append(sota_context.research_context)
            if sota_context_list:
                sota_str = "\n".join(sota_context_list)
                sota_result = f"Remember, you are the Best Expert \
            in this domain, and here is some short summery of \
            the backgroud knowledge,they are state-of-the-art knowledges, \
                which can help do the deep anaylsis:{sota_str}"

        logger.info(f"Load domain SOTA knowledge: {sota_result}")
        return sota_result

    def _parse_response(self, response) -> dict:
        # è§£æ OpenAI çš„å“åº”å¹¶æå–è¯„åˆ†ç»“æœ, é»˜è®¤è¿”å›ç»“æœéƒ½æ˜¯json æ ¼å¼ï¼Œé™¤å»```json
        if not response or not hasattr(response, "output_text"):
            raise ValueError("Invalid response from OpenAI API.")
        # convert it to json date type
        try:
            # Remove optional 'json' identifier after the opening triple backticks
            response_text = response.output_text
            cleaned_result = re.sub(r"^```json\s*", "", response_text)
            # Remove closing triple backticks
            cleaned_result = re.sub(r"```$", "", cleaned_result)
            # TODO: å®é™…ä¸Šè¿˜å¯ä»¥è€ƒè™‘å¢å¼ºä¸€äº›é€»è¾‘ï¼Œå¤„ç†å¼‚å¸¸æƒ…å†µï¼Œæé«˜å®¹é”™èƒ½åŠ›ã€‚æ¯”å¦‚ai çš„åˆ†æ•°æ˜¯ä¸æ˜¯åœ¨ 0-10 ä¹‹é—´ï¼Ÿ

            json_result = json.loads(cleaned_result)
        except Exception as e:
            raise ValueError(f"AI response is not a valid json date. Error: {e}")

        return json_result

    def _handle_retry(self, attempt: int):
        """
        å¤„ç†é‡è¯•é€»è¾‘ï¼Œä¾‹å¦‚è®°å½•æ—¥å¿—æˆ–ç­‰å¾…ä¸€æ®µæ—¶é—´ã€‚
        """
        wait_time = 2**attempt  # æŒ‡æ•°é€€é¿ç­–ç•¥ï¼š2, 4 ç§’
        print(f"Attempt {attempt + 1} failed. Retrying after {wait_time} seconds...")
        time.sleep(wait_time)


class TopicSummaryAssistant(BaseAssistant):
    def __init__(self, config: dict):
        super().__init__(config)
        self.name = config.get("name")
        self.model_name = config.get("model_name")
        self.prompt = config.get("prompt")
        self.instruction = config.get("instruction")

    def do_work(self, publication, context):
        if publication.research_topics and publication.keywords:
            # å¦‚æœè®ºæ–‡å·²ç»åŒ…å«ç ”ç©¶è¯¾é¢˜,then ç›´æ¥ä½¿ç”¨
            logger.info(
                f"found existing keywords:{publication.keywords} and key research topic: {publication.research_topics}"
            )
            return {
                "keywords": publication.keywords,
                "research_topics": publication.research_topics,
            }

        if publication.keywords:
            keywords = publication.keywords
        elif publication.title:
            keywords = (
                "I dont have keywords, but I have the title: " + publication.title
            )
        else:
            keywords = (
                "I dont have keywords, and I dont have title. Please try your best."
            )
        logger.info(f"preparing prompt with keywords:{keywords}")
        prompt = self.prompt.format(
            title=publication.title,
            keywords=keywords,
            abstract=publication.abstract,
            conclusion=publication.conclusion,
        )

        logger.info(f"Prompt for Topic Summary Assistant: {prompt[:200]}")
        return self._get_response(publication=publication, prompt=prompt)


class TraigeAssistant(BaseAssistant):
    def __init__(self, config: dict):
        super().__init__(config)
        self.name = config.get("name")
        self.model_name = config.get("model_name")
        self.prompt = config.get("prompt")
        self.instruction = config.get("instruction")

    def do_work(self, publication, context):
        """ "
        è®ºæ–‡æ ‡é¢˜ï¼š{title}
        å…³é”®å­—ï¼š{keywords}
        æ‘˜è¦ï¼š{abstract}
        ç»“è®ºï¼š{conclusion}
        æ–‡ç« å…¨æ–‡ï¼š{full_text}
        ä¸ºäº†å¸®åŠ©ä½ æ›´å¥½åœ°å›ç­”é—®é¢˜ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›è¯¥é¢†åŸŸçš„å‰æ²¿ç ”ç©¶æˆæœï¼ˆSOTAï¼‰ï¼š
        {sota_context}
        """
        # get the sota context
        sota_context = self._load_domain_sota_knowledge(publication)
        # build the prompt
        prompt = self.prompt.format(
            title=publication.title,
            keywords=publication.research_topics,
            abstract=publication.abstract,
            conclusion=publication.conclusion,
            full_text=publication.content_raw_text[:10000],  # TODOæµ‹è¯•é˜¶æ®µï¼Œé™åˆ¶é•¿åº¦
            sota_context=sota_context,
        )
        logger.info(f"Prompt for Triage Assistant: {prompt[:200]}")
        return self._get_response(publication=publication, prompt=prompt)


class DomainExpertReviewAssistant(BaseAssistant):
    def __init__(self, config: dict):
        super().__init__(config)
        self.name = config.get("name")
        self.model_name = config.get("model_name")
        self.prompt = config.get("prompt")
        self.instruction = config.get("instruction")

    def do_work(self, publication, traige_summary, previous_review_json) -> dict:
        # update the instruction, TODOï¼š æ„Ÿè§‰åº”è¯¥æœ‰æ›´å¥½çš„åœ°æ–¹æ¥å¤„ç†è¿™ä¸ªé€»è¾‘
        if self.name == AIAssistantType.REVIEWER_GENERAL:
            self.instruction = self.instruction.format(
                domain=publication.research_topics
            )
        else:
            # å¦‚æœæ˜¯domainçš„ä¸“å®¶ï¼Œè¯·ä¸“å®¶è‡ªå·±æ¥åŠ è½½ä»–çš„ä¸“ä¸šé¢†åŸŸçŸ¥è¯†, TODO: å¾…è¡¥å……è€ƒè™‘ï¼Œç°åœ¨ç›´æ¥pass
            pass
        # build the prompt
        if not traige_summary:
            traige_summary = "I dont have enough context, please try your best."
        if not previous_review_json:
            previous_review_json = "So far, you are the first expert to review and score this paper. please try your best to give a review and put an score to each domain."

        prompt = self.prompt.format(
            title=publication.title,
            keywords=publication.research_topics,
            abstract=publication.abstract,
            conclusion=publication.conclusion,
            traige_summary=traige_summary,
            previous_review_json=previous_review_json,
        )

        logger.info(
            f"Prompt for Domain Expert [{self.name}] Review Assistant: {prompt[:200]}"
        )
        return self._get_response(publication=publication, prompt=prompt)


class DailyReportAssistant(BaseAssistant):
    def __init__(self, config: dict):
        super().__init__(config)
        self.name = config.get("name")
        self.model_name = config.get("model_name")
        self.prompt = config.get("prompt")
        self.instruction = config.get("instruction")

    def do_work(self, day_date: date, top_k: int, context: str):
        if not day_date:
            day_date = datetime.now().date()
        # ä»¥ä¸‹æ˜¯{date}çš„Top {top_k} è®ºæ–‡çš„æ€»ç»“ä¿¡æ¯ï¼Œæ•°æ®ä¸ºJsonæ ¼å¼ï¼š
        #            {paper_data}
        prompt = self.prompt.format(
            date=str(day_date), top_k=top_k, paper_data=str(context)
        )

        logger.info(f"Prompt for Topic Summary Assistant: {prompt[:200]}")
        return self._get_response(publication=None, prompt=prompt)


class AIAssistantType(Enum):
    PAPER_TRIAGE = "paper_triage"
    TOPIC_SUMMARY = "topic_summary"
    REVIEWER_GENERAL = "reviewer_general"
    DOMAIN_REVIEWER_ALGORITHM = "domain_reviewer_algorithm"
    DOMAIN_REVIEWER_ARCHITECT = "domain_reviewer_architect"
    DOMAIN_REVIEWER_CLUSTER = "domain_reviewer_cluster"
    DOMAIN_REVIEWER_CHIP = "domain_reviewer_chip"
    DOMAIN_REVIEWER_NETWORK = "domain_reviewer_network"
    DAILY_REPORT_SUMMARY = "daily_report_summary"
    # xxx_domain å¯ä»¥æ³¨å…¥æ›´å¤šå®šåˆ¶åŒ–çš„ä¸“å®¶


class PaperReviewConfig:
    SECTION_TITLES = {
        "abstract": {"abstract"},
        "introduction": {"introduction", "background", "preliminaries", "preliminary"},
        "related_work": {
            "related work",
            "prior work",
            "literature review",
            "related studies",
        },
        "methodology": {
            "methodology",
            "methods",
            "method",
            "approach",
            "proposed method",
            "model",
            "architecture",
            "framework",
            "algorithm",
            "system design",
        },
        "experiment": {
            "experiment",
            "experiments",
            "experimental setup",
            "experiment setup",
            "setup",
            "implementation details",
            "evaluation",
            "evaluation setup",
        },
        "ablation": {"ablation", "ablation study"},
        "results": {
            "results",
            "performance",
            "findings",
            "observations",
            "empirical results",
        },
        "summary": {"summary"},
        "conclusion": {"conclusion", "final remarks", "closing remarks", "discussion"},
        "limitations": {"limitations"},
        "acknowledgments": {
            "acknowledgments",
            "acknowledgements",
            "funding",
            "author contributions",
        },
        "references": {"references", "bibliography", "cited works"},
        "appendix": {
            "appendix",
            "supplementary material",
            "supplementary",
            "additional materials",
        },
    }

    # è®ºæ–‡æ‹†åˆ†å‚æ•°
    MAX_LINE_PER_CHUNK = 25  # æ¯ä¸ª chunk æœ€å¤§è¡Œæ•°
    OVERLAP_LINES = 5  # ä¸Šä¸‹æ–‡é‡å è¡Œæ•°
    REFERENCE_BLOCK_SIZE = 8  # å‚è€ƒæ–‡çŒ®å—å¤§å°ï¼ˆ5~10è¡Œï¼‰

    GPT_MODEL_NAME = "gpt-4o-mini"

    ai_assistants_config = {
        AIAssistantType.TOPIC_SUMMARY: {
            "name": "topic_summary",
            "model_name": "gpt-4o-mini",
            "instruction": """You are a professional academic paper analysis expert with extensive experience in evaluating research papers. Your task is to read and analyze the provided core information of a research paperâ€”including its title, keywords (if available), abstract, and conclusion. Based on this information, please extract the following:
                            1. **Core Keywords:** Summarize the key technical terms that encapsulate the main themes of the paper.
                            2. **Research Topics:** Identify the top 3 specific research topics or directions that the paper addresses. These topics should be more detailed and specific than the keywords. For example, if a keyword is "cache," a more specific research topic might be "compression methods for key-value caches."
                            Return your answer strictly in JSON format, with no extra text or annotations.""",
            "prompt": """Please evaluate the following research paper information and extract the required details:
                    Paper Information:
                    Title: {title}
                    Keywords: {keywords}
                    Abstract: {abstract}
                    Conclusion: {conclusion}
                    Based on the above information, please extract:
                    1. Core Keywords (i.e., the key technical terms that summarize the paper's main themes).
                    2. The Top 3 specific research topics or directions that the paper addresses.

                    Requirements:
                    - Use technical terminology without translating into another language.
                    - Keep the content concise.
                    - Return your answer strictly in the following JSON format, ensuring it is syntactically correct and contains no additional text or annotations:
                    {{
                        "keywords": ["keyword1", "keyword2", "keyword3", ...],
                        "research_topics": ["research_topic1", "research_topic2", "research_topic3"]
                    }}""",
        },
        AIAssistantType.PAPER_TRIAGE: {
            "name": "paper_triage",
            "model_name": "gpt-4o-mini",
            "instruction": """You are a seasoned academic paper reviewer, well-versed in the cutting-edge research trends in both academia and industry. I will provide you with the core information of a research paperâ€”including its title, keywords, abstract, conclusion, and author details.

                    Based on this information, please analyze and answer the following six questions systematically:
                    For each of the following questions, your answer must be extremely detailedâ€”approximately 300 words or more for each question if necessaryâ€”to ensure that your response contains the most critical and valuable information:
                    1. What is the core problem that the paper aims to solve?
                    2. What are the main technical contributions of the paper?
                    3. What innovative aspects or proposals does the paper present that are noteworthy?
                    4. Does the paper compare its approach with current state-of-the-art (SOTA) research in the field? If yes, what conclusions does it draw from this comparison?
                    5. Who are the authors of the paper, and what are their affiliated institutions? Please list them as completely as possible.
                    6. How would you assess the influence of the authors and their institutions in academia or industry? Consider whether they are associated with renowned universities, laboratories, or industrial research centers, and whether their work is widely cited or applied.

                    Please ensure your answers are accurate, professional, and strictly based on the provided paper content and background context.
                    After answering the questions, return your final evaluation in strict JSON format without any extra text or markdown formatting. The JSON must follow the exact structure shown below:
                    {{
                    "core_problem": "Provide a detailed explanation of the core problem addressed by the paper.",
                    "technical_contributions": "Provide a detailed explanation of the paper's main technical contributions.",
                    "innovations_and_proposals": [
                        "List and detail the first innovative aspect or proposal.",
                        "List and detail the second innovative aspect or proposal."
                    ],
                    "sota_comparison": "Describe whether and how the paper compares its approach with current SOTA research and state the comparison conclusions.",
                    "authors_and_affiliations": {{
                        "authors": ["Author1", "Author2"],
                        "institutions": ["Institution1", "Institution2"]
                    }},
                    "influence_assessment": "Provide a detailed assessment of the academic or industrial influence of the authors and their institutions."
                    }}""",
            "prompt": """Please evaluate the following research paper based on the instruction above. Use the paperâ€™s core information to answer each of the six questions in detail, ensuring that your response is comprehensive and precise.

                        Paper Information:
                        Title: {title}
                        Keywords: {keywords}
                        Abstract: {abstract}
                        Conclusion: {conclusion}
                        Full Text: {full_text}
                        
                        For additional context, here are some state-of-the-art (SOTA) research outcomes in this field:
                        {sota_context}

                        Requirements:
                        - Return your response strictly in the JSON format as specified in the instruction.
                        - Do not include any extra text or markdown formatting.
                        - Ensure that the JSON is syntactically correct and can be parsed by a machine.
                        """,
        },
        AIAssistantType.REVIEWER_GENERAL: {
            "name": "reviewer_general",
            "model_name": "gpt-4o-mini",
            "instruction": """You are a seasoned expert in {domain}. You will receive the core information of a research paper, including the title, keywords (if available), abstract, and conclusion. Based on this information and the provided background context, you are required to evaluate the paper on the following five dimensions. All scores should be on a scale from 1.0 to 10.0 (scores may include up to two decimal places), where 10.0 represents the best performance. Please strictly adhere to the provided paper content and context to ensure a fair and unbiased evaluation.
                        For each of the five dimensions, please provide a detailed answer of approximately 300 words or more, explaining your reasoning thoroughly.
                        1. **Technical Innovation:** Evaluate the novelty of the method compared to existing solutions. For instance, determine whether the paper introduces a completely new mechanism or replaces traditional components with an entirely new architecture. If the work opens up a new direction or shifts community focus to a new research paradigm, a high score (above 8.0) is warranted.
                        2. **Performance Improvement:** Examine the experimental results on recognized benchmark datasets and compare them with the best previously published results. A significant improvement in key metrics should result in a high score. Consider whether the improvements are consistent across multiple tasks or datasets, demonstrating both general applicability and strong performance gains.
                        3. **Theoretical/Engineering Simplicity & Efficiency:** Assess whether the method reduces computational complexity or engineering difficulty. For example, determine if the algorithm decreases time complexity, reduces model parameters or memory usage, or shortens training/inference time. High scores are justified if the work simplifies the model architecture while maintaining or improving performance.
                        4. **Reusability & Impact:** Consider the general applicability and influence of the work. If the paper provides a model structure or algorithm that is easily transferable to other tasks and is widely adopted, it should receive a high score. Conversely, if the method is overly specialized or complex, leading to limited adoption, a lower score is appropriate.
                        5. **Author & Institutional Authority:** Review the backgrounds of the authors and their institutions. If the authors are affiliated with top-tier research institutions or have a history of influential work, and if the institution has a strong research tradition, this dimension should receive a higher score.
                        After evaluating these dimensions, please provide a recommendation on whether the paper should be read ("yes" or "no") along with your reasoning. If the paper primarily aggregates existing information without adding significant new value, you may assign low scores and explicitly state that it is not recommended.
                        Finally, please assign a confidence level to your overall evaluation on a scale from 0.0 (completely uncertain) to 1.0 (very certain), based on your analysis.
                        Return your final result in the following JSON format. Do not include any extra information or annotations, and ensure the JSON is strictly valid and provided in plain text (without any Markdown formatting):
                        {{
                            "dimensions": {{
                                "innovation": {{""score": 7.5, "reason": "Detailed explanation of the paper's technical innovation (approximately 150 words or more)."}},
                                "performance":{{""score": 5.5, "reason": "Detailed explanation of the paper's performance improvement (approximately 150 words or more)."}},
                                "simplicity": {{""score": 6.1, "reason": "Detailed explanation of the paper's performance improvement (approximately 150 words or more)."}},
                                "reusability":{{""score": 5.5, "reason": "Detailed explanation of the paper's performance improvement (approximately 150 words or more)."}},
                                "authority":{{""score": 8.0,"Detailed explanation of the paper's reusability and impact (approximately 150 words or more)."}}
                                }},
                            "recommend": "yes",
                            "reason": "A detailed explanation of why the paper is recommended or not, with sufficient detail."
                            "who_should_read": "Target audience description. "
                            "confidence": 0.85
                        }}""",
            "prompt": """Please evaluate the following research paper based on the instruction provided above. Provide detailed responses for each of the evaluation dimensions, with each explanation being around 300 words or more, so that the answer is thoroughly justified.ï¼š
                        Paper Information:
                        Title: {title}
                        Keywords: {keywords}
                        Abstract: {abstract}
                        Conclusion: {conclusion}
                        Additional Summary/QA: {traige_summary}
                        Based on the above information, perform your evaluation according to the instruction, and return your answer strictly in the JSON format shown in the instruction. Do not include any additional text or Markdown formatting.""",
        },
        AIAssistantType.DOMAIN_REVIEWER_ALGORITHM: {
            "name": "domain_reviewer_algorithm",
            "model_name": "gpt-4o-mini",
            "instruction": """You are a seasoned expert in the field of Computer Software and Algorithm Research, with extensive experience in reviewing academic papers. You have received the core information of a research paper along with preliminary scores provided by a general AI Reviewer for your reference.

                        Please follow these steps:

                        1. **Relevance Check:**  
                        - First, determine whether the paper is closely related to your domain (Computer Software and Algorithm Research).  
                        - If the paper is not relevant, do not modify the existing scores. Instead, return the original scores and add a confidence value (e.g., 0.4) indicating that the paper is not within your field.

                        2. **Score Review and Adjustment:**  
                        - If the paper is within your domain, carefully review the preliminary scores provided by the general reviewer.  
                        - Based on your professional judgment, perform a verification or slight adjustment of the scores.  
                        - Avoid making major changes unless you have strong professional justification. If you do modify any score, briefly explain the reason.  
                        - The evaluation dimensions include:  
                            - Innovation  
                            - Performance Improvement  
                            - Simplicity & Efficiency  
                            - Reusability & Impact  
                            - Authority (Authors and Institutions)
                        - For each of the five dimensions, please provide a detailed answer of approximately 300 words or more, explaining your reasoning thoroughly.

                        3. **Recommendation and Confidence:**  
                        - Confirm or update the recommendation (recommend) and the target readership (who_should_read).  
                        - Finally, provide a confidence level (ranging from 0.0 to 1.0, where 1.0 indicates very high confidence) in your overall evaluation.

                        âš ï¸ **Important:**  
                        - Return your final result strictly in JSON format as specified below. Do not include any additional text or commentary beyond the JSON output.
                        - Ensure that the JSON is syntactically correct and machine-parsable.

                        The expected JSON format is as follows:
                        {{
                            "dimensions": {{
                                "innovation": {{""score": 7.5, "reason": "Detailed explanation of the paper's technical innovation (approximately 150 words or more)."}},
                                "performance":{{""score": 5.5, "reason": "Detailed explanation of the paper's performance improvement (approximately 150 words or more)."}},
                                "simplicity": {{""score": 6.1, "reason": "Detailed explanation of the paper's performance improvement (approximately 150 words or more)."}},
                                "reusability":{{""score": 5.5, "reason": "Detailed explanation of the paper's performance improvement (approximately 150 words or more)."}},
                                "authority":{{""score": 8.0,"Detailed explanation of the paper's reusability and impact (approximately 150 words or more)."}}
                                }},
                            "recommend": "yes",
                            "reason": "A detailed explanation of why the paper is recommended or not, with sufficient detail."
                            "who_should_read": "Target audience description. "
                            "confidence": 0.85
                        }}""",
            "prompt": """Please evaluate the following research paper based on the instruction provided above. You have been given the paper's core information along with the preliminary scores from the general AI Reviewer.

                        Paper Information:
                        Title: {title}
                        Keywords: {keywords}
                        Abstract: {abstract}
                        Conclusion: {conclusion}
                        Additional Summary/QA: {traige_summary}

                        Preliminary Scores (for reference):
                        {previous_review_json}
                        
                        Instructions:
                        - First, determine if the paper is relevant to your domain (Computer Software and Algorithm Research).  
                        - If it is not relevant, simply return the preliminary scores, appending a confidence value (for example, 0.4) to indicate that the paper is outside your domain.
                        - If the paper is relevant, carefully review the preliminary scores and adjust them slightly only if necessary. Avoid large changes unless absolutely required, and provide brief reasons for any modifications.
                        - Also, confirm or update the recommendation and the intended readership.
                        - Finally, assign an overall confidence level (from 0.0 to 1.0) to your evaluation.

                        Return your response strictly in the following JSON format (do not include any additional text or Markdown formatting)""",
        },
        AIAssistantType.DOMAIN_REVIEWER_ARCHITECT: {
            "name": "domain_reviewer_architect",
            "model_name": "gpt-4o-mini",
            "instruction": """You are a seasoned expert in the field of Computer Architecture, with extensive experience in reviewing academic papers and assessing state-of-the-art research in hardware design, microarchitecture, and system performance. You have received the core information of a research paper along with preliminary scores provided by a general AI Reviewer for your reference.

                        Please follow these steps:

                        1. **Relevance Check:**  
                        - First, determine whether the paper is closely related to your domain (Computer Architecture).  
                        - If the paper is not relevant, do not modify the existing scores. Instead, return the original scores and append a confidence value (e.g., 0.4) indicating that the paper is outside your field.

                        2. **Score Review and Adjustment:**  
                        - If the paper is relevant, carefully review the preliminary scores provided by the general reviewer.  
                        - Based on your professional judgment in the context of computer architecture, verify or slightly adjust the scores.  
                        - Avoid making major changes unless you have strong professional justification. If you modify any score, provide a brief explanation of your reasoning.  
                        - The evaluation dimensions include:  
                            - **Innovation:** Evaluate the novelty of the architectural design or hardware innovation (e.g., new microarchitecture, novel hardware acceleration, energy efficiency improvements).  
                            - **Performance Improvement:** Assess the improvements in performance metrics such as throughput, latency, or power efficiency compared to existing solutions.  
                            - **Simplicity & Efficiency:** Consider whether the proposed architecture simplifies design complexity, reduces resource usage, or improves system efficiency.  
                            - **Reusability & Impact:** Evaluate how adaptable the architecture is to different workloads or systems and its potential impact on the research community and industry.  
                            - **Authority (Authors and Institutions):** Consider the reputation and track record of the authors and their institutions.

                        - For each of the five dimensions, please provide a detailed explanation (approximately 300 words or more if necessary) to thoroughly justify your reasoning.

                        3. **Recommendation and Confidence:**  
                        - Confirm or update the recommendation (recommend) and the intended readership (who_should_read).  
                        - Finally, assign an overall confidence level (ranging from 0.0 to 1.0, where 1.0 indicates very high confidence) in your evaluation.

                        âš ï¸ **Important:**  
                        - Return your final result strictly in JSON format as specified below. Do not include any additional text or commentary beyond the JSON output.  
                        - Ensure that the JSON is syntactically correct and machine-parsable.

                        The expected JSON format is as follows:
                        {{
                            "dimensions": {{
                                "innovation": {{""score": 7.5, "reason": "Detailed explanation of the paper's technical innovation (approximately 150 words or more)."}},
                                "performance":{{""score": 5.5, "reason": "Detailed explanation of the paper's performance improvement (approximately 150 words or more)."}},
                                "simplicity": {{""score": 6.1, "reason": "Detailed explanation of the paper's performance improvement (approximately 150 words or more)."}},
                                "reusability":{{""score": 5.5, "reason": "Detailed explanation of the paper's performance improvement (approximately 150 words or more)."}},
                                "authority":{{""score": 8.0,"Detailed explanation of the paper's reusability and impact (approximately 150 words or more)."}}
                                }},
                            "recommend": "yes",
                            "reason": "A detailed explanation of why the paper is recommended or not, with sufficient detail."
                            "who_should_read": "Target audience description. "
                            "confidence": 0.85
                        }}""",
            "prompt": """Please evaluate the following research paper based on the instruction provided above. You have been given the paper's core information along with the preliminary scores from the general AI Reviewer.

                        Paper Information:
                        Title: {title}
                        Keywords: {keywords}
                        Abstract: {abstract}
                        Conclusion: {conclusion}
                        Additional Summary/QA: {traige_summary}

                        Preliminary Scores (for reference):
                        {previous_review_json}
                        
                        Instructions:
                        - First, determine if the paper is relevant to your domain (Computer Software and Algorithm Research).  
                        - If it is not relevant, simply return the preliminary scores, appending a confidence value (for example, 0.4) to indicate that the paper is outside your domain.
                        - If the paper is relevant, carefully review the preliminary scores and adjust them slightly only if necessary. Avoid large changes unless absolutely required, and provide brief reasons for any modifications.
                        - Also, confirm or update the recommendation and the intended readership.
                        - Finally, assign an overall confidence level (from 0.0 to 1.0) to your evaluation.

                        Return your response strictly in the following JSON format (do not include any additional text or Markdown formatting)""",
        },
        # TODO: å…¶ä»–é¢†åŸŸçš„è¯„å®¡åŠ©æ‰‹é…ç½®
        AIAssistantType.DAILY_REPORT_SUMMARY: {
            "name": "daily_report_summary",
            "model_name": "gpt-4o",
            "instruction": """Your task is to generate a concise, easy-to-read daily report summarizing the key insights from the top N research papers of the day. The final report should be written in Markdown format and presented as a JSON object, enabling readers to quickly understand the takeaways and feel compelled to explore more.
                    Steps:
                        1.	Global Analysis:
                        â€¢	Theme Extraction: Analyze the provided JSON data to identify the dominant themes. Extract 3â€“5 primary topics from the abstracts and triage QA fields.
                        â€¢	Innovation Clustering: Review the â€œinnovation_reasonâ€ field to highlight clusters of technological innovations and emerging methods.
                        2.	Highlight Extraction:
                        â€¢	Breakthrough Methods: Identify papers with an innovation_score of 8 or higher that introduce breakthrough methods.
                        â€¢	Authoritative Research: Flag papers with an authority_score of 9 or above, especially those with renowned authors.
                        â€¢	Reusability Potential: Highlight papers with a reusability_score of 8 or above, indicating strong potential for reuse.
                        3.	Quality and Feasibility Comparison:
                        â€¢	Performance Comparison: Compare the dataset quality by examining SOTA comparisons in the performance_reason field.
                        â€¢	Engineering Feasibility: Analyze the distribution of simplicity_score to assess how practical and implementable the methods are.
                        â€¢	Anomaly Detection: Flag any papers with a confidence_score below 0.8 as potential anomalies that may require further scrutiny.
                        4.	Report Composition:
                        â€¢	Structure & Style: Compose a concise Markdown report that includes:
                        â€¢	A brief summary table with key metrics.
                        â€¢	Hyperlinks to the highlighted papers.
                        â€¢	A clear conclusion with key takeaways, using emojis (ğŸ’¥, ğŸš€, ğŸ”¥) to emphasize important points.
                        â€¢	Tone: Combine academic rigor with an engaging, modern style. Keep the language crisp and directâ€”this is an introductory briefing meant to quickly inform and attract further reading, not a detailed technical analysis.

                    Output Format:

                    Return your result strictly as a JSON object with the following structure:
                    {{
                        "day_date": "YYYY-MM-DD",
                        "markdown_report": "Your concise daily report in markdown format"
                    }}""",
            "prompt": """
                    Below is the preliminary JSON summary of the top {top_k} papers for {date}:
                    {paper_data}
                    
                    In addition to the information above, please ensure your report is informative and preciseâ€”avoid vague or generic statements. You should consider the following points in your report:
                        1.	Recommendation Rationale (in less than 144 words): Include key innovation highlights, practical value, and authoritative endorsements.
                        2.	Provocative Questions (5 in total): Formulate one question each about methodology, application scenarios, and the overall impact on the field.
                        3.	Curiosity Hook: Use a â€œWhat ifâ€¦?â€ style sentence to spark interest.

                    Your final output should be a concise Markdown report, returned as JSON in the following structure:
                    {{
                        "day_date": "YYYY-MM-DD",
                        "markdown_report": "Your concise daily report in markdown format"
                    }}
                    """,
        },
    }


class ReviewArxivPaper:
    """
    1. åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼Œä¼šå¤šæ¬¡å‘é€è¯·æ±‚
    2. å¯¹å•ä¸€è®ºæ–‡ï¼Œé¦–å…ˆæ£€æŸ¥è®ºæ–‡ PDF æ˜¯å¦è¢«ä¸‹è½½ï¼Œå¦‚æœæ²¡æœ‰ï¼Œå°±å¼€å¯ä¸‹è½½
    3. å¯¹äºä¸‹è½½å¥½çš„è®ºæ–‡ï¼Œå¼€å§‹è§£æ PDFï¼Œè½¬ä¸ºæ–‡æœ¬ï¼Œé’ˆå¯¹æ–‡æœ¬ç‰‡æ®µå¼€å§‹è°ƒç”¨ OpenAI çš„æ¥å£åšåˆ†æ
    4. å®šä¹‰ 5 ä¸ªä¸åŒçš„ AI åŠ©æ‰‹ï¼ŒåŸºäºæ–‡æœ¬ï¼Œå¯¹è®ºæ–‡å¼€å§‹æ‰“åˆ†
    """

    def __init__(self):
        """
        åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
        """
        self.client = OpenAI()

    def _get_db(self):
        db = SessionLocal()
        try:
            yield db
        finally:
            db.close()

    def process(self, paper: "ArxivPaper") -> PaperScores:
        """
        å¤„ç†å•ç¯‡è®ºæ–‡
        """
        logger.info(f"Processing paper: â€œ{paper.title}â€")
        db = None
        try:

            # æ£€æŸ¥ PDF æ˜¯å¦å·²ç»ä¸‹è½½
            full_path, relative_path = self._get_pdf_path(paper)
            if not self._is_pdf_downloaded(paper):
                logger.info(f"PDF not downloaded, downloading now: {paper.pdf_url}")
                full_path, relative_path = self._download_pdf(paper)

            if not full_path or not relative_path:
                logger.error(f"Failed to locate the PDF for paper: {paper.title}")
                return None
            logger.info(f"Found the paper PDF file in: {relative_path}")

            # æ£€æŸ¥ Publication æ˜¯å¦å·²ç»å­˜åœ¨
            db = next(self._get_db())
            publication = (
                db.query(Publication)
                .filter(Publication.paper_id == paper.arxiv_id)
                .first()
            )
            if not publication:
                logger.info(
                    f"Publication not found in database, creating new entry for: {paper.title}"
                )
                # 1. ç®€å•å¤„ç†ï¼Œå»æ‰ç©ºè¡Œ
                text = self._parse_pdf_to_text(full_path)
                text_lines = [line.strip() for line in text.split("\n") if line.strip()]
                # 2. è®ºæ–‡çš„ä¸€äº›å…¸å‹è¯†åˆ«æ ‡é¢˜
                title_indices = self._detect_section_titles(text_lines)
                # 3. æŒ‰æ ‡é¢˜æ‹†åˆ†
                section_chunks = self._split_to_chunks_by_title(
                    text_lines, title_indices
                )
                # 4. é€‰å‡ºé‡ç‚¹å†…å®¹
                abstract_text = "\n".join(
                    section_chunks.get("abstract", "")
                )  # convert to string
                conclusion_text = "\n".join(section_chunks.get("conclusion", ""))
                references_text = "\n".join(section_chunks.get("references", ""))
                if title_indices.get("refernces"):
                    # main context å–åˆ° references ä¹‹å‰çš„æ‰€æœ‰å†…å®¹
                    main_context = "\n".join(
                        text_lines[0 : section_chunks.get("references")]
                    )
                else:
                    # å¦‚æœæ²¡æœ‰ referencesï¼Œç®€å•å–å…¨æ–‡
                    main_context = text
                logger.info(f"abstract_text: {abstract_text[:100]}")
                logger.info(f"conclusion_text: {conclusion_text[:100]}")
                logger.info(f"Main context extracted from PDF: {main_context[:200]}...")
                # æ–°å»ºä¸€ä¸ªpublication
                publication = Publication(
                    paper_id=paper.arxiv_id,
                    instance_id=0,  # é»˜è®¤å…³è”åˆ°ä¸€ä¸ªéä¼šè®®
                    title=self._clean_db_str_input(paper.title),
                    year=paper.published.strftime("%Y"),
                    publish_date=paper.published,
                    tldr="",
                    abstract=self._clean_db_str_input(
                        paper.summary if paper.summary else abstract_text
                    )[
                        :5000
                    ],  # TODO: æœ‰æ²¡æœ‰æ›´å¥½çš„åŠæ³•å¤„ç†ï¼Œå½“å‰é»˜è®¤conclusion/abstractåœ¨5000å­—ç¬¦ä»¥å†…
                    conclusion=self._clean_db_str_input(conclusion_text)[
                        :5000
                    ],  # TODO: æœ‰æ²¡æœ‰æ›´å¥½çš„åŠæ³•å¤„ç†ï¼Œå½“å‰é»˜è®¤conclusion/abstractåœ¨5000å­—ç¬¦ä»¥å†…
                    content_raw_text=self._clean_db_str_input(main_context),
                    reference_raw_text=self._clean_db_str_input(references_text),
                    pdf_path=relative_path,  # å­˜å‚¨ç›¸å¯¹è·¯å¾„ï¼Œæ–¹ä¾¿ç³»ç»Ÿè¿ç§»
                    citation_count=0,
                    award="",
                    doi="",
                    url="",
                    pdf_url=paper.pdf_url,
                    attachment_url="",
                )
                # å…ˆä¿å­˜åˆ°æ•°æ®åº“
                db.add(publication)
                logger.info(f"Saving publication to database: {publication.title}")
                db.commit()
                db.refresh(publication)

            # ç„¶åäº¤ç»™è¯„å®¡æ‰“åˆ†
            scores = self._review_paper_with_ai_experts(publication)
            db.add(publication)
            db.add(scores)
            db.commit()

            db.refresh(scores)
            return scores

        except SQLAlchemyError as db_err:
            db.rollback()
            logger.error(
                f"Database error occurred while processing paper â€œ{paper.title}â€: {db_err}"
            )
        except Exception as e:
            logger.error(
                f"An unexpected error occurred while processing paper â€œ{paper.title}â€: {e}"
            )
        finally:
            if db:
                db.close()

        return None

    def process_batch(self, paper_list: List["ArxivPaper"]) -> List[PaperScores]:
        """
        æ‰¹é‡å¤„ç†è®ºæ–‡
        """
        # TODO: ä¸€æ¬¡å¤„ç†50ç¯‡è®ºæ–‡ï¼Œè¯•è¿è¡Œç¨³å®šååˆ é™¤
        paper_list = paper_list[:50]
        results = []
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        max_threads = 4  # æ ¹æ®æ‚¨çš„ç³»ç»Ÿå’Œä»»åŠ¡éœ€æ±‚è°ƒæ•´çº¿ç¨‹æ•°
        with ThreadPoolExecutor(max_threads) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_paper = {
                executor.submit(self.process, paper): paper for paper in paper_list
            }
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in as_completed(future_to_paper):
                paper = future_to_paper[future]
                try:
                    results.append(future.result())  # è·å–çº¿ç¨‹æ‰§è¡Œç»“æœ
                except Exception as exc:
                    logger.error(f"{paper} å¤„ç†æ—¶å‘ç”Ÿå¼‚å¸¸: {exc}")

        return results

    def get_ai_daily_report(self, report_day: date, top_k: int, context: str) -> str:
        # è°ƒç”¨å¤§æ¨¡å‹æ¥æ’°å†™æ¯æ—¥æŠ¥å‘Š TODO: è¿™ä¸ªå‡½æ•°ä¸åº”è¯¥æ”¾åœ¨è¿™ä¸ªç±»ï¼Œå¾…æ”¹è¿›
        # step 1: æ˜¯å¦æ•°æ®åº“å·²ç»ç”Ÿæˆäº†ï¼Œå¦‚æœæœ‰ï¼Œå°±ç›´æ¥è¿”å›
        # step 2: è°ƒç”¨å¤§æ¨¡å‹æ¥åš
        daily_summary_config = PaperReviewConfig.ai_assistants_config[
            AIAssistantType.DAILY_REPORT_SUMMARY
        ]
        daily_report_assistant = DailyReportAssistant(daily_summary_config)
        logger.info(
            f"calling {daily_report_assistant.name} to generate daily report for date: {report_day}"
        )
        report_result = daily_report_assistant.do_work(
            day_date=report_day, top_k=top_k, context=context
        )
        return report_result

    def _detect_section_titles(self, lines):
        """æ£€æµ‹æ ‡é¢˜è¡Œï¼Œè¿”å›æ ‡é¢˜è¡Œç´¢å¼•"""
        title_indices = {}
        section_num = 0  # å¯¹äº summaryã€limitation è¿™ç±»æ ‡é¢˜ï¼Œå¯èƒ½åœ¨å¤šä¸ªä½ç½®å‡ºç°ï¼Œéœ€è¦å…¨éƒ¨ä¿ç•™ã€‚åŠ ä¸€ä¸ªç¼–ç ï¼Œä¿è¯ç‹¬ç«‹
        for i, line in enumerate(lines):
            clean_line = re.sub(r"[^a-zA-Z\s]", "", line).strip().lower()
            for section, keywords in PaperReviewConfig.SECTION_TITLES.items():
                if any(
                    re.match(
                        rf"^\s*(\d+([\.\-ï¼‰]\d+)*[\.\-ï¼‰]*\s*)?{re.escape(kw)}\s*$",
                        clean_line,
                        re.IGNORECASE,
                    )
                    for kw in keywords
                ):
                    if section in title_indices:
                        title_indices[section + "_" + str(section_num)] = i
                        section_num += 1
                    else:
                        title_indices[section] = i
                    break
        return title_indices

    def _split_to_chunks_by_title(self, lines, title_indices):
        """æŒ‰ç…§æ ‡é¢˜è¡Œåˆ†å—"""
        chunks = defaultdict(list)
        sorted_sections = sorted(
            title_indices.items(), key=lambda x: x[1]
        )  # æŒ‰å‡ºç°é¡ºåºæ’åº

        for idx, (section, start_idx) in enumerate(sorted_sections):
            end_idx = (
                sorted_sections[idx + 1][1]
                if idx + 1 < len(sorted_sections)
                else len(lines)
            )
            chunks[section] = lines[start_idx:end_idx]

        return chunks

    def _get_pdf_path(self, paper):
        """
        æ ¹æ®è®ºæ–‡ä¿¡æ¯ç”Ÿæˆå¯¹åº”çš„ PDF å­˜å‚¨è·¯å¾„ã€‚
        å‡è®¾ paper.date æ˜¯ 'YYYY-MM-DD' æ ¼å¼çš„å­—ç¬¦ä¸²ï¼Œpaper.arxiv_id æ˜¯è®ºæ–‡çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚
        """
        try:
            # è§£ææ—¥æœŸå­—ç¬¦ä¸²
            year = paper.published.strftime("%Y")
            month = paper.published.strftime("%m")
            # æ„å»ºç›¸å¯¹è·¯å¾„
            relative_path = f"./pdf/{year}/{month}/{paper.arxiv_id}.pdf"
            # ä»é…ç½®æ–‡ä»¶ä¸­è·å– PDF å­˜å‚¨ç›®å½•
            data_storage_dir = get_data_storage_dir()
            # æ„å»ºå®Œæ•´è·¯å¾„
            full_path = data_storage_dir / relative_path

            logger.info(f"search PDF relative path: {relative_path}")
            # å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º
            full_path.parent.mkdir(parents=True, exist_ok=True)

        except Exception as e:
            logger.error(f"Error generating PDF path: {e}")
            raise e
        # è¿”å› PDF å­˜å‚¨ç›®å½•å’Œæ–‡ä»¶è·¯å¾„
        return str(full_path), str(relative_path)

    def _is_pdf_downloaded(self, paper: "ArxivPaper") -> bool:
        """
        æ£€æŸ¥è®ºæ–‡çš„ PDF æ˜¯å¦å·²ä¸‹è½½
        """
        # å®ç°æ£€æŸ¥é€»è¾‘
        logger.info(f"Checking if PDF is downloaded for paper: {paper.title}")
        full_path, relative_path = self._get_pdf_path(paper)
        logger.info(f"Checking if PDF exists at: {relative_path}")
        return os.path.exists(full_path)

    def _download_pdf(self, paper: "ArxivPaper"):
        """
        ä¸‹è½½è®ºæ–‡çš„ PDF å¹¶ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„ã€‚
        """
        full_path, relative_path = self._get_pdf_path(paper)
        try:
            response = requests.get(paper.pdf_url, timeout=10)  # è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º10ç§’
            response.raise_for_status()  # å¦‚æœå“åº”çŠ¶æ€ç ä¸æ˜¯ 200ï¼Œå°†å¼•å‘ HTTPError å¼‚å¸¸
            with open(full_path, "wb") as f:
                f.write(response.content)
            logging.info(f"PDF downloaded and saved to {relative_path}")
            return full_path, relative_path
        except requests.HTTPError as http_err:
            logging.error(f"HTTP error occurred while downloading PDF: {http_err}")
        except ConnectionError as conn_err:
            logging.error(
                f"Connection error occurred while downloading PDF: {conn_err}"
            )
        except Timeout as timeout_err:
            logging.error(
                f"Timeout error occurred while downloading PDF: {timeout_err}"
            )
        except requests.RequestException as req_err:
            logging.error(f"An error occurred while downloading PDF: {req_err}")
        except IOError as io_err:
            logging.error(f"File operation error: {io_err}")
        return None

    def _parse_pdf_to_text(self, pdf_path) -> str:
        """
        è§£æ PDF å¹¶å°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬
        """
        doc = fitz.open(pdf_path)
        text = ""
        for page in doc:
            # print(page.number, page.get_text()[:20])
            text += page.get_text()
        logger.info(
            f"PDF parsed to text, page count{len(doc)}, text length: {len(text)}"
        )
        return text

    def _review_paper_with_ai_experts(self, publication: Publication) -> PaperScores:
        """
        ä½¿ç”¨ OpenAI æ¥å£åˆ†ææ–‡æœ¬ï¼Œå¹¶æ ¹æ®ä¸åŒæ ‡å‡†è¿›è¡Œè¯„åˆ†
        """
        try:
            # 0. å…ˆè®©è®ºæ–‡æ€»ç»“ç ”ç©¶çš„å…³é”®æ–¹å‘
            topic_summary_config = PaperReviewConfig.ai_assistants_config[
                AIAssistantType.TOPIC_SUMMARY
            ]
            topic_summary_assistant = TopicSummaryAssistant(topic_summary_config)
            logger.info(
                f"calling {topic_summary_assistant} to process paper {publication.paper_id}"
            )
            topic_result = topic_summary_assistant.do_work(publication, context="")
            logger.info(f"Topic summary result: {topic_result}")

            # 1. è®©AIæ€»ç»“è®ºæ–‡çš„å‡ ä¸ªå…³é”®é—®é¢˜, ä»¥åŠç­”æ¡ˆï¼Œè¾…åŠ©æ¨ç†
            if topic_result:
                publication.keywords = topic_result.get("keywords", [])
                publication.research_topics = topic_result.get("research_topics", [])

            traige_assistant_config = PaperReviewConfig.ai_assistants_config[
                AIAssistantType.PAPER_TRIAGE
            ]
            traige_assistant = TraigeAssistant(traige_assistant_config)
            context = ""
            if publication.research_topics:
                context = (
                    f"Here are some key research topics: {publication.research_topics}"
                )
            traige_summary = traige_assistant.do_work(publication, context)
            # save traige result
            if traige_summary:
                publication.triage_qa = traige_summary
            logger.info(f"Triaging result: {traige_summary}")

            # 2. è®©AI æ ¹æ®åˆæ­¥çš„ä¿¡æ¯æ¥æ‰“åˆ†ï¼Œè‡ªåŠ¨æ£€ç´¢ç›¸å…³æ ¸å¿ƒé—®é¢˜çš„state of the art ç ”ç©¶æˆæœä½œä¸ºè¡¥å……åˆ¤æ–­
            reviwer_general = PaperReviewConfig.ai_assistants_config[
                AIAssistantType.REVIEWER_GENERAL
            ]
            reviewer_general_assistant = DomainExpertReviewAssistant(reviwer_general)
            init_score = reviewer_general_assistant.do_work(
                publication, traige_summary=traige_summary, previous_review_json=""
            )
            logger.info(f"Initial score is: {init_score}")

            # 2.1. å¤„ç†åˆæ­¥è¯„åˆ†ç»“æœ
            score = PaperScores(
                paper_id=publication.paper_id,
                title=publication.title,
                ai_reviewer=reviewer_general_assistant.name,
                review_status="pending",
                error_message="",
                log=f"AI reviewer {reviewer_general_assistant.name} reviewed the paper and provided initial score.",
            )
            score = self._assign_score_values(score=score, json_score=init_score)
            score.review_status = "completed"
            logger.info(f"sucessfully init a score object: {score}")

            # 3: éå†é¢†åŸŸä¸“å®¶ï¼Œå¯¹æ‰“åˆ†è¿›è¡Œæ ¸æŸ¥å’Œä¿®æ­£
            domain_reviwers = self._load_domain_review_assistants()
            for expert_name, expert_assistant in domain_reviwers.items():
                logger.info(
                    f"Processing paperâ€œ {publication.paper_id} â€ with expert {expert_name}"
                )
                expert_score = expert_assistant.do_work(
                    publication,
                    traige_summary=traige_summary,
                    previous_review_json=init_score,
                )
                if expert_score and expert_score.get("confidence"):
                    # åˆ¤æ–­å’Œåˆå¹¶ä¸“å®¶çš„ç»“æœ
                    if expert_score.get("confidence") > score.confidence_score:
                        logger.info(
                            f"Expert {expert_name} provided a valid review/updates."
                        )
                        score = self._assign_score_values(
                            score=score, json_score=expert_score
                        )
                        score.ai_reviewer = expert_assistant.name
                        score.review_status = "rewrited by expert"
                        score.error_message = ""
                        score.log = "\n".join(
                            [
                                score.log,
                                f"Expert {expert_name} reviewed the paper and provided update: {score.get_review_status()}",
                            ]
                        )
                    else:
                        score.log = "\n".join(
                            [
                                score.log,
                                f"Expert {expert_name} reviewed the paper, but confidence is low.",
                            ]
                        )
                else:
                    score.log = "\n".join(
                        [
                            score.log,
                            f"Expert {expert_name} reviewed the paper but no valid score provided.",
                        ]
                    )

            logger.info(
                f"Congratulate, AI experts reviewed the paper and the final status are: {score.get_review_status()}"
            )
            return score
        except Exception as e:
            logger.error(f"Error processing paper â€œ{publication.paper_id}â€: {e}")
            raise e

    def _assign_score_values(self, score: PaperScores, json_score: dict):
        if not json_score or not json_score.get("dimensions"):
            raise ValueError("Invalid json data, unable to retrieve score data.")
        dimensions = json_score.get("dimensions")
        score.innovation_score = (
            0.0
            if not dimensions.get("innovation")
            else dimensions.get("innovation").get("score", 0.0)
        )
        score.innovation_reason = (
            "N/A"
            if not dimensions.get("innovation")
            else dimensions.get("innovation").get("reason", "N/A")
        )
        score.performance_score = (
            0.0
            if not dimensions.get("performance")
            else dimensions.get("performance").get("score", 0.0)
        )
        score.performance_reason = (
            "N/A"
            if not dimensions.get("performance")
            else dimensions.get("performance").get("reason", "N/A")
        )
        score.simplicity_score = (
            0.0
            if not dimensions.get("simplicity")
            else dimensions.get("simplicity").get("score", 0.0)
        )
        score.simplicity_reason = (
            "N/A"
            if not dimensions.get("simplicity")
            else dimensions.get("simplicity").get("reason", "N/A")
        )
        score.reusability_score = (
            0.0
            if not dimensions.get("reusability")
            else dimensions.get("reusability").get("score", 0.0)
        )
        score.reusability_reason = (
            "N/A"
            if not dimensions.get("reusability")
            else dimensions.get("reusability").get("reason", "N/A")
        )
        score.authority_score = (
            0.0
            if not dimensions.get("authority")
            else dimensions.get("authority").get("score", 0.0)
        )
        score.authority_reason = (
            "N/A"
            if not dimensions.get("authority")
            else dimensions.get("authority").get("reason", "N/A")
        )

        # Calculate the weighted score and round it to 2 decimal places
        score.weighted_score = round(
            0.35 * score.innovation_score
            + 0.25 * score.performance_score
            + 0.15 * score.simplicity_score
            + 0.15 * score.reusability_score
            + 0.1 * score.authority_score,
            2,
        )
        # Assign other fields directly from the JSON
        score.recommend = str(json_score.get("recommend", False)).lower() in (
            "yes",
            "true",
            "1",
        )
        score.recommend_reason = json_score.get("reason", "")
        score.who_should_read = json_score.get("who_should_read", "")
        score.confidence_score = json_score.get("confidence", 0.0)

        return score

    def _load_domain_review_assistants(
        self,
    ) -> Dict[AIAssistantType, DomainExpertReviewAssistant]:
        assistants = {}
        # åªåŠ è½½domain_xxxçš„ä¸“å®¶ã€‚ TODOï¼š è¿™é‡Œçš„é€»è¾‘å¯ä»¥å†ä¼˜åŒ–ï¼Œå½“å‰ç®€å•æŒ‰å­—ç¬¦ä¸²è¿‡æ»¤
        for assistant_type, config in PaperReviewConfig.ai_assistants_config.items():
            if not assistant_type.value.lower().startswith("domain_reviewer"):
                continue
            assistants[assistant_type] = DomainExpertReviewAssistant(config)
            logger.info(f"Loaded domain review assistants: {assistant_type}'")
        return assistants

    def _clean_db_str_input(self, text: str):
        if text is None:
            return ""
        # Remove NUL characters
        text = text.replace("\x00", "")
        return text
